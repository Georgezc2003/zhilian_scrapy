# -*- coding: utf-8 -*-
# @Time    : 2017/1/7
# @Author  : Sunyan Gu
# @Site    : NJUPT


from scrapy.spiders import CrawlSpider,Rule
from scrapy.selector import Selector
from scrapy.http import Request
from zhilianzhaopingstep1.items import Zhilianzhaopingstep1Item
import re
from zhilianzhaopingstep1.first_configure import FirstConfigure
import datetime


class zhilianSpider(CrawlSpider):
    name = 'zhilian_step1'

    firstConfigure = FirstConfigure()  # 初始类实例
    start_urls = firstConfigure.create_url() # 起始url的获得

    # ？有用吗headers？ 伪装成浏览器
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
        'Cookie': 'sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%22158a570e1f4524-0630e0dd6ca1fc-5c4f231c-2073600-158a570e1f58e4%22%2C%22props%22%3A%7B%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%7D%7D; dywez=95841923.1481543197.6.3.dywecsr=other|dyweccn=121124451|dywecmd=cnt; metroopen=0; notlogin=1; _zg=%7B%22uuid%22%3A%20%221596206e4bc9f-05a41ff602a077-5d4e211f-1fa400-1596206e4bd86e%22%2C%22sid%22%3A%201483408336.062%2C%22updated%22%3A%201483409337.614%2C%22info%22%3A%201483408336065%7D; __xsptplus30=30.5.1483751939.1483752824.2%233%7Cgraph.qq.com%7C%7C%7C%7C%23%23zMgznXAT5HqL1xmCpo5FyRwbRr4QWVCX%23; LastCity%5Fid=532; LastCity=%e6%b2%b3%e5%8c%97; _jzqckmp=1; JSSearchModel=0; _jzqx=1.1480244430.1483941230.4.jzqsr=jobs%2Ezhaopin%2Ecom|jzqct=/xining/sj659/.jzqsr=sou%2Ezhaopin%2Ecom|jzqct=/jobs/searchresult%2Eashx; jobtypeopen=1; firstchannelurl=https%3A//passport.zhaopin.com/account/login%3FbkUrl%3Dhttp%253A%252F%252Fsou.zhaopin.com%252Fjobs%252Fsearchresult.ashx%253Fispts%253D1%2526isfilter%253D1%2526p%253D1%2526bj%253D4082000%2526sj%253D2175; urlfrom=121126445; urlfrom2=121126445; adfcid=none; adfcid2=none; adfbid=0; adfbid2=0; __utmt=1; JsNewlogin=2035029440; JSloginnamecookie=18801583533; JSShowname=%e9%a1%be%e5%ad%99%e7%82%8e; at=6f482ec4a3104559b5e3f609d95a5e13; Token=6f482ec4a3104559b5e3f609d95a5e13; rt=32eed09347204bccafdfe6cb2453f065; JSpUserInfo=24342E6955715D7943320B754A6A5F71076A5868456B4F7409333979246B4C345B6950715379443202754C6A5371076A5C68426B4A7409332079246B4C3414F1312AE5094F327675346A5671056A5868426B4174063347795C6B49345A695F712B7905324275576A08715B6A04684A6B2A74663348795B6B4A342B693C71567945321E75406A4B71066A59684B6B4C7400334E792B6B3D3457695971507921327275446A2171796A5E68496B4A74063346795B6B45345C695D71507921326775446A5A710F6A3A68386B447403334E793F6B2134246955710879163252754F6A5C71056A5168476B4B740B334D795D6B1334526909715879433203751F6A5B71026A5968126B4A74533313795A6B40340C6959715A79153208758; uiioit=213671340F69436B5A6A507947644774053505325D755D6D51683B7420734936053409698; lastchannelurl=https%3A//passport.zhaopin.com/account/login%3FbkUrl%3Dhttp%253A%252F%252Fjobs.zhaopin.com%252F456094211250032.htm; Hm_lvt_38ba284938d5eddca645bb5e02a02006=1482847729,1483406511,1483751849,1483955363; Hm_lpvt_38ba284938d5eddca645bb5e02a02006=1483966360; loginreleased=1; LastJobTag=%e8%8a%82%e6%97%a5%e7%a6%8f%e5%88%a9%7c%e4%ba%94%e9%99%a9%e4%b8%80%e9%87%91%7c%e7%bb%a9%e6%95%88%e5%a5%96%e9%87%91%7c%e5%b8%a6%e8%96%aa%e5%b9%b4%e5%81%87%7c%e5%91%98%e5%b7%a5%e6%97%85%e6%b8%b8%7c%e5%85%a8%e5%8b%a4%e5%a5%96%7c%e9%a4%90%e8%a1%a5%7c%e4%ba%a4%e9%80%9a%e8%a1%a5%e5%8a%a9%7c%e9%80%9a%e8%ae%af%e8%a1%a5%e8%b4%b4%7c%e5%8c%85%e4%bd%8f%7c%e5%8a%a0%e7%8f%ad%e8%a1%a5%e5%8a%a9%7c%e5%bc%b9%e6%80%a7%e5%b7%a5%e4%bd%9c%7c%e5%ae%9a%e6%9c%9f%e4%bd%93%e6%a3%80%7c%e5%8c%85%e5%90%83%7c%e5%b9%b4%e5%ba%95%e5%8f%8c%e8%96%aa%7c%e5%b9%b4%e7%bb%88%e5%88%86%e7%ba%a2%7c%e8%a1%a5%e5%85%85%e5%8c%bb%e7%96%97%e4%bf%9d%e9%99%a9%7c%e9%ab%98%e6%b8%a9%e8%a1%a5%e8%b4%b4%7c%e5%85%8d%e8%b4%b9%e7%8f%ad%e8%bd%a6%7c%e6%88%bf%e8%a1%a5%7c%e8%82%a1%e7%a5%a8%e6%9c%9f%e6%9d%83%7c%e9%87%87%e6%9a%96%e8%a1%a5%e8%b4%b4; LastSearchHistory=%7b%22Id%22%3a%220a972eb0-24cb-43c8-b708-865c0d3c22d3%22%2c%22Name%22%3a%22%e6%b2%b3%e5%8c%97+%2b+%e5%b8%82%e5%9c%ba+%2b+%e5%b8%82%e5%9c%ba%e4%b8%93%e5%91%98%2f%e5%8a%a9%e7%90%86%22%2c%22SearchUrl%22%3a%22http%3a%2f%2fsou.zhaopin.com%2fjobs%2fsearchresult.ashx%3fispts%3d1%26isfilter%3d1%26p%3d1%26bj%3d4082000%26sj%3d171%22%2c%22SaveTime%22%3a%22%5c%2fDate(1483966362155%2b0800)%5c%2f%22%7d; SubscibeCaptcha=D581E43F8DA1E0FADD638C8103D7E943; dywea=95841923.4129866369659275300.1480244388.1483958334.1483962658.36; dywec=95841923; dyweb=95841923.50.9.1483966346266; __utma=269921210.177375426.1480244388.1483958334.1483962658.36; __utmb=269921210.50.9.1483966346268; __utmc=269921210; __utmz=269921210.1481543197.6.3.utmcsr=other|utmccn=121124451|utmcmd=cnt; _qzja=1.2044009355.1481260065167.1483954819479.1483962686038.1483966368288.1483966368729.0.0.0.365.23; _qzjb=1.1483962686038.31.0.0.0; _qzjc=1; _qzjto=41.4.0; _jzqa=1.411727141887181000.1480244430.1483958334.1483962686.32; _jzqc=1; _jzqb=1.46.10.1483962686.1'
    }

    # 被parse()调用，传参是如何完成的呢？ 还涉及callback函数，像是利用response，好像是框架系统定义的函数。
    def parse_item(self, response):

        selector = Selector(response)  # 为什么不可以直接用response.xpath,如parse()
        job_name = selector.xpath('//div[@class="inner-left fl"]/h1/text()').extract()[0]
        company_name = selector.xpath('//div[@class="inner-left fl"]/h2/a/text()').extract()[0]
        company_introduction_list = selector.xpath('//div[@class="tab-inner-cont"]/p/span/text()').extract()
        company_introduction = ''
        for i in company_introduction_list:
            company_introduction += i.strip() + '\n'
        recruitment_link = response.url
        job_description_list = selector.xpath('//div[@class="tab-inner-cont"]/p/text()').extract()
        job_description = ''
        for i in job_description_list:
            a = i.strip()
            if len(a) > 0:
                job_description += a + '\n'

        welfare = selector.xpath('//div[@class="welfare-tab-box"]/span/text()').extract()
        job_profile = {}
        job_profile['job_category'] = selector.xpath('//ul[@class="terminal-ul clearfix"]/li[4]/strong/text()').extract()[0].strip()
        job_profile['monthly_salary'] = selector.xpath('//ul[@class="terminal-ul clearfix"]/li[1]/strong/text()').extract()[0].strip()
        job_profile['job_type'] = selector.xpath('//ul[@class="terminal-ul clearfix"]/li[8]/strong/a/text()').extract()[0].strip()
        job_profile['minimum_education'] = selector.xpath('//ul[@class="terminal-ul clearfix"]/li[6]/strong/text()').extract()[0].strip()
        job_profile['recruiting_number'] = selector.xpath('//ul[@class="terminal-ul clearfix"]/li[7]/strong/text()').extract()[0].strip()
        try:
            job_profile['release_date'] = selector.xpath('//ul[@class="terminal-ul clearfix"]/li[3]/strong/span/text()').extract()[0].strip()
        except:
            job_profile['release_date'] = selector.xpath('//ul[@class="terminal-ul clearfix"]/li[3]/strong/text()').extract()[0].strip()

        job_profile['work_experience'] = selector.xpath('//ul[@class="terminal-ul clearfix"]/li[5]/strong/text()').extract()[0].strip()
        job_profile['work_place'] = selector.xpath('//ul[@class="terminal-ul clearfix"]/li[2]/strong/a/text()').extract()[0].strip()

        company_profile = {}
        profiles = selector.xpath('//ul[@class="terminal-ul clearfix terminal-company mt20"]/li').extract()

        for i in profiles:
            profile = re.findall('<span>(.*?)</span>',i.strip(),re.S)[0]
            # print('///////////////',profile)
            if profile == '公司行业：':
                try:
                    company_profile[profile] = re.findall('">(.*?)</a>',i.strip(),re.S)[0]
                except:
                    company_profile[profile] = re.findall('<strong>(.*?)</strong>',i.strip(),re.S)[0]
            elif profile == '公司规模：':
                company_profile[profile] = re.findall('<strong>(.*?)</strong>', i.strip(), re.S)[0]
            elif profile == '公司性质：':
                company_profile[profile] = re.findall('<strong>(.*?)</strong>', i.strip(), re.S)[0]
            elif profile == '公司主页：':
                company_profile[profile] = re.findall('blank">(.*?)</a>', i.strip(), re.S)[0]
            elif profile == '公司地址：':
                address = re.findall('<strong>(.*?)</strong>', i.strip(), re.S)[0].strip()
                if address[-4:] == '<br>':
                    address = address[:-4]
                company_profile[profile] = address

        now = datetime.datetime.now()
        time = now.strftime('%Y-%m-%d')

        item = response.meta['item']
        item['company_introduction'] = company_introduction
        item['company_name'] = company_name
        item['company_profile'] = company_profile
        item['job_description'] = job_description
        item['release_date'] = job_profile['release_date']
        item['work_place'] = job_profile['work_place']
        item['job_category'] = job_profile['job_category']
        item['job_type'] = job_profile['job_type']
        item['job_name'] = job_name
        item['job_profile'] = job_profile
        item['recruitment_link'] = recruitment_link
        item['welfare'] = welfare
        item['time'] = time
        item['company_job_name'] = company_name+' '+job_name+' '+job_profile['work_place']
        # makeLog.log4("爬取%s的招聘信息! ", recruitment_link)
        yield item

    # 官网：parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。
    # 该方法负责解析返回的数据(response data)，提取数据(生成item)  以及  生成需要进一步处理的URL的 Request 对象。
    def parse(self, response):
        item = Zhilianzhaopingstep1Item()
        # 感觉response返回的是查询页面
        item_urls = response.xpath('//td[@class="zwmc"]/div/a/@href').extract() # 找到该页面全部职位url链接共60个
        # 计数器，在某一查询条件下总的职位数量，如2698
        job_cnt = response.xpath('//span[@class="search_yx_tj"]/em/text()').extract()[0]
        job_cnt = int(job_cnt)

        if job_cnt > 60:  # 应该是一页60个职位数量，？？item_urls的数量和job_cnt为什么会不一致？？
            for item_url in item_urls:  # 下面是生成需要进一步处理的URL的 Request 对象
                yield Request(item_url,callback=self.parse_item,meta={'item': item},headers=self.headers)
            try:  # 下面代码：当本页url全部发送request之后，寻找下一页新的页面
                next_url = response.xpath('//li[@class="pagesDown-pos"]/a/@href').extract()[0]
                yield Request(next_url,headers=self.headers)
            except:
                pass
        else:
            for item_url in item_urls[:job_cnt]:
                yield Request(item_url, callback=self.parse_item, meta={'item': item},headers=self.headers)

            try:
                next_url = response.xpath('//li[@class="pagesDown-pos"]/a/@href').extract()[0]
                yield Request(next_url,headers=self.headers)
            except:
                pass
